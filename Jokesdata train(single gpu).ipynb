{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f0481fb-4326-4091-8fc4-f4e0e844a4e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "def read_documents_from_directory(directory):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8ac9791-750b-4fe1-afcd-031ebce004c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = 'TrainingData'\n",
    "text_data = read_documents_from_directory(train_directory)\n",
    "text_data = re.sub(r'\\n+', '\\n', text_data).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30184bc8-e4bb-4934-8cb9-581fd778002c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"DataSource/final_train.txt\", \"w\") as f:\n",
    "    f.write(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4ee4d84-b63f-43b8-ab5e-a4f2855df807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba593535-e8ba-4c5e-87c8-fba13601e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "85efecdc-b64b-41dd-a436-16438a366c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb948111-1c7a-4e23-b458-daa98499f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def train(train_file_path, model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps,\n",
    "          stop_token=\"###\",\n",
    "          device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [stop_token]})\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name).to(torch.device(device))\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef448501-0f51-4dce-9081-035da5fa7b7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_file_path = \"TrainingData/final_train.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'nmodels'\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 4\n",
    "num_train_epochs = 1000\n",
    "save_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "720b714a-56ed-4ee0-8302-abd13b982f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:57, Epoch 1000/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory nmodels/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory nmodels/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd374a5e-d020-412c-84b8-15b45d721c93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69f21307-9258-40cf-862e-d412a6ae34ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "def generate_text(model_path, sequence, max_length,stop_token=\"<Stop>\"):\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=tokenizer.encode(stop_token)[0] if stop_token else None,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    data_gen=tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "    str1=\"\"\n",
    "    for x in data_gen.split(\"...\"):\n",
    "      # if x==\"...\":\n",
    "      #   break\n",
    "      # elif str1==\"\":\n",
    "      #   str1=x\n",
    "      # else:\n",
    "      #   str1=str1+\" \"+x\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "514f2064-0059-44d8-afed-e56589bb7a1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q] Why does Santa have three gardens?\n",
      "[A] He has a lot of water in his belly.\n",
      "[Q] What's the difference between a corn husker with epilepsy and a hooker with dysentery?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"nmodels\"\n",
    "sequence = \"[Q] Why does Santa have three gardens?\"\n",
    "max_len = 50\n",
    "generate_text(model_path, sequence, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9791f-5735-4053-a5f4-8fc9c89b0c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
