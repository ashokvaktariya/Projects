{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b16b46-3f98-451d-9cff-5ab1bb9ab6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e730f23-8faf-4976-9e81-6c884b577e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4691e841-7254-45cb-9b77-74b06d350490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9312cab5-44ff-4fad-9934-26648060030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_file_path, model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps,\n",
    "          stop_token=\"###\",\n",
    "          num_gpus=2): \n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [stop_token]})\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "    )\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    def train_func(config):\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=TrainingArguments(\n",
    "                output_dir=output_dir,\n",
    "                overwrite_output_dir=overwrite_output_dir,\n",
    "                per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "                num_train_epochs=config[\"num_train_epochs\"],\n",
    "            ),\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "    config_space = {\n",
    "        \"per_device_train_batch_size\": ray.tune.choice([2, 4, 8]),\n",
    "        \"num_train_epochs\": ray.tune.choice([1, 2, 3]),\n",
    "    }\n",
    "    pbt_scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"eval_loss\",  # Change it to your evaluation metric\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=2,\n",
    "        hyperparam_mutations=config_space,\n",
    "    )\n",
    "    analysis = ray.tune.run(\n",
    "        train_func,\n",
    "        config=config_space,\n",
    "        scheduler=pbt_scheduler,\n",
    "        stop={\"training_iteration\": 10},  # Adjust stopping criteria\n",
    "        num_samples=4,  # Number of trials\n",
    "        resources_per_trial={\"gpu\": num_gpus},  # Specify the number of GPUs\n",
    "    )\n",
    "    ray.actor.exit_actor()\n",
    "    print(\"Best config:\", analysis.get_best_config(metric=\"eval_loss\"))\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106999f6-7177-4407-98d7-babed9656e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "2024-01-23 11:11:25,246\tINFO worker.py:1558 -- Calling ray.init() again after it has already been called.\n",
      "2024-01-23 11:11:25,249\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py:744: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-23 11:20:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:08:57.70        </td></tr>\n",
       "<tr><td>Memory:      </td><td>68.1/2015.5 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Logical resource usage: 0/40 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:H100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  num_train_epochs</th><th style=\"text-align: right;\">  per_device_train_bat\n",
       "ch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_func_2782d_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">4</td></tr>\n",
       "<tr><td>train_func_2782d_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">4</td></tr>\n",
       "<tr><td>train_func_2782d_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">2</td></tr>\n",
       "<tr><td>train_func_2782d_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  train(\n",
    "        train_file_path=\"data/train_10.txt\",\n",
    "        model_name=\"gpt2\",\n",
    "        output_dir=\"distributed_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=100,\n",
    "        stop_token=\"###\",\n",
    "        num_gpus=2 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff2927-e508-4e18-85cf-b55277545aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
